{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Program:\tXorNN\n",
    "\n",
    "Utility:\n",
    "        A Tensorflow Neural Network trained to classify whether an or statement is inclusive or exclusive\n",
    "\n",
    "WARNING! This program leverages and NVIDIA Graphics card tp speed things up. This program will be very slow on a cpu only system\n",
    "\n",
    "\tAuthor:\t\t\tTroi Chua\n",
    "\tDate:\t\t\tMay 7, 2017\n",
    "\tCollaborators:\n",
    "\tCitations:\t\t\n",
    "\t\thttps://github.com/soerendip/Tensorflow-binary-classification\n",
    "\t\thttps://keras.io/preprocessing/text/\n",
    "\t\thttp://www.nltk.org/\n",
    "\t\thttps://www.tensorflow.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports python3 functionality for python2\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "# numpy is the most popular matrix manipulation library in python and a lot of libraries use their data structures\n",
    "import numpy as np\n",
    "# import the machine library\n",
    "import tensorflow as tf\n",
    "\n",
    "# a library for timing how long the program takes\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start the timer\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# these load the data into the program\n",
    "train_txt = open(\"Data_1/word/train/fpWordRep_trainDAT.txt\", \"r\")\n",
    "test_txt = open(\"Data_1/word/test/fpWordRep_testDAT.txt\", \"r\")\n",
    "train_labels_txt = open(\"Data_1/labels/train_targets.txt\", \"r\")\n",
    "test_labels_txt = open(\"Data_1/labels/test_targets.txt\", \"r\")\n",
    "\n",
    "# these parse the data and organize them such that 1 word is in each partition of an array\n",
    "train_list = train_txt.readlines()\n",
    "test_list = test_txt.readlines()\n",
    "train_labels_list = train_labels_txt.readlines()\n",
    "test_labels_list = test_labels_txt.readlines()\n",
    "\n",
    "# these convert the string values stored in the text file to numbers and formats them into a dataset acceptatble by TF\n",
    "# -1 in reshape is for an unknown size\n",
    "training_data = [np.asarray([float(n) for n in line.split()]).reshape(-1,299) for line in train_list]\n",
    "testing_data = [np.asarray([float(n) for n in line.split()]).reshape(-1,299) for line in test_list]\n",
    "training_labels = [np.asarray([float(n) for n in line.split()]).reshape(-1,2) for line in train_labels_list]\n",
    "testing_labels = [np.asarray([float(n) for n in line.split()]).reshape(-1,2) for line in test_labels_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate   = 0.01\t# literally the rate the network learns at;the rate the optimizer minimzes loss.\n",
    "batch_size = 20\t\t\t# the size of one batch of data that the network learns on so it may learn in increments\n",
    "training_epochs = 5000\t# the number of times the network is trained\n",
    "display_step    = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1  = 20\t# 1st hidden layer of neurons\n",
    "n_hidden_2  = 20\t# 2nd hidden layer of neurons\n",
    "n_input     = 299\t# number of words per sentence\n",
    "n_classes\t= 2\t\t# inclusive = [0,1], exclusive = [1,0]\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=1e-4)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=1e-4)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes],stddev=1e-4))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "# the network architecture\n",
    "def network(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # the output (predicted value)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder variables\n",
    "x = tf.placeholder(tf.float32, [None,n_input])\n",
    "y = tf.placeholder(tf.float32, [None,n_classes])\n",
    "\n",
    "# Construct model\n",
    "pred = network(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t 0000 \tcost = 14.913790669\n",
      "Epoch:\t 0100 \tcost = 13.110060973\n",
      "Epoch:\t 0200 \tcost = 10.539969895\n",
      "Epoch:\t 0300 \tcost = 11.053034461\n",
      "Epoch:\t 0400 \tcost = 8.933832981\n",
      "Epoch:\t 0500 \tcost = 3.460888496\n",
      "Epoch:\t 0600 \tcost = 4.042564182\n",
      "Epoch:\t 0700 \tcost = 6.208289301\n",
      "Epoch:\t 0800 \tcost = 5.815868158\n",
      "Epoch:\t 0900 \tcost = 1.482488853\n",
      "Epoch:\t 1000 \tcost = 2.956894216\n",
      "Epoch:\t 1100 \tcost = 1.475718902\n",
      "Epoch:\t 1200 \tcost = 2.623286192\n",
      "Epoch:\t 1300 \tcost = 0.122336594\n",
      "Epoch:\t 1400 \tcost = 0.051246965\n",
      "Epoch:\t 1500 \tcost = 0.042799297\n",
      "Epoch:\t 1600 \tcost = 0.222631110\n",
      "Epoch:\t 1700 \tcost = 0.284935316\n",
      "Epoch:\t 1800 \tcost = 0.009345830\n",
      "Epoch:\t 1900 \tcost = 0.146004372\n",
      "Epoch:\t 2000 \tcost = 0.028366978\n",
      "Epoch:\t 2100 \tcost = 0.004873103\n",
      "Epoch:\t 2200 \tcost = 0.001781388\n",
      "Epoch:\t 2300 \tcost = 0.008963984\n",
      "Epoch:\t 2400 \tcost = 0.193265223\n",
      "Epoch:\t 2500 \tcost = 0.067680098\n",
      "Epoch:\t 2600 \tcost = 0.001920011\n",
      "Epoch:\t 2700 \tcost = 1.249782446\n",
      "Epoch:\t 2800 \tcost = 0.024471254\n",
      "Epoch:\t 2900 \tcost = 0.032094141\n",
      "Epoch:\t 3000 \tcost = 0.014096130\n",
      "Epoch:\t 3100 \tcost = 0.018965523\n",
      "Epoch:\t 3200 \tcost = 0.018484739\n",
      "Epoch:\t 3300 \tcost = 0.004679030\n",
      "Epoch:\t 3400 \tcost = 0.013222686\n",
      "Epoch:\t 3500 \tcost = 0.007498654\n",
      "Epoch:\t 3600 \tcost = 0.011155368\n",
      "Epoch:\t 3700 \tcost = 0.020644954\n",
      "Epoch:\t 3800 \tcost = 0.033608130\n",
      "Epoch:\t 3900 \tcost = 0.061385329\n",
      "Epoch:\t 4000 \tcost = 0.019021950\n",
      "Epoch:\t 4100 \tcost = 0.015747285\n",
      "Epoch:\t 4200 \tcost = 0.048875265\n",
      "Epoch:\t 4300 \tcost = 0.102445295\n",
      "Epoch:\t 4400 \tcost = 0.128177871\n",
      "Epoch:\t 4500 \tcost = 0.009353232\n",
      "Epoch:\t 4600 \tcost = 0.000127695\n",
      "Epoch:\t 4700 \tcost = 0.000584806\n",
      "Epoch:\t 4800 \tcost = 0.001976363\n",
      "Epoch:\t 4900 \tcost = 8.351188426\n",
      "\n",
      "Training Finished!\n",
      "\n",
      "Predicted Output\t\tTargetOutput\tSuccess\n",
      "\n",
      "[[-0.7255277  2.6169219]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[ 0.65560621  1.2358458 ]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[-2.52510405  4.4150176 ]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[ 19.63041687 -17.74172974]]\t[[ 0.  1.]]\t[False]\n",
      "\n",
      "[[ 23.05901337 -21.16949081]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[ -8.89565468  10.78823376]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[-25.43022919  27.32462692]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[ 1.52228713  0.36942452]]\t[[ 1.  0.]]\t[False]\n",
      "\n",
      "[[ 18.25425911 -16.36951256]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[-14.52036285  16.41236877]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[-6.79827213  8.69252396]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[ 23.65815163 -21.76966476]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[ 10.03676128  -8.14736557]]\t[[ 0.  1.]]\t[False]\n",
      "\n",
      "[[ 13.4502182  -11.55937195]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[-34.61435318  36.50701904]]\t[[ 0.  1.]]\t[ True]\n",
      "\n",
      "[[ 10.43195343  -8.53463745]]\t[[ 1.  0.]]\t[ True]\n",
      "\n",
      "[[ 19.98820114 -18.10055351]]\t[[ 0.  1.]]\t[False]\n",
      "\n",
      "[[-12.49778366  14.3935957 ]]\t[[ 1.  0.]]\t[False]\n",
      "\n",
      "[[ 3.96291065 -2.07194042]]\t[[ 0.  1.]]\t[False]\n",
      "\n",
      "[[-30.93797302  32.82911301]]\t[[ 1.  0.]]\t[False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Session = execute the code\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(tf.global_variables_initializer())\t# needed to initialize all the TF data structures\n",
    "\n",
    "\t# Training\n",
    "\tfor epoch in range(training_epochs):\n",
    "\t\t\n",
    "\t\tavg_cost = 0\t# initalize the cost\n",
    "\t\ttotal_batches = int(len(training_data) / batch_size)\t\n",
    "\n",
    "\t\t# total batches is how many paritions the data is partition into\n",
    "\t\ttrDat_batches = np.array_split(training_data, total_batches)\n",
    "\t\ttrLab_batches = np.array_split(training_labels, total_batches)\n",
    "\n",
    "\t\tfor i in range(total_batches):\t# iterates through all the batches\n",
    "\t\t\tfor j in range(len(trDat_batches[0])):\t# This is needed to iterate through all the data in each batch\n",
    "\t\t\t\t\n",
    "\t\t\t\tbatch_Dat, batch_Lab = trDat_batches[i][j], trLab_batches[i][j]\t\n",
    "\n",
    "\t\t\t\t# Feed the network the current sentence\n",
    "\t\t\t\t_, c = sess.run([optimizer, cost], feed_dict={x:\tbatch_Dat,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ty:\tbatch_Lab})\n",
    "\n",
    "\t\t\t\t# Calculate the avg error of the current training iteration\n",
    "\t\t\t\tavg_cost += c / total_batches\n",
    "\t\t\n",
    "\n",
    "\t\t# will print the current status of training every display step\n",
    "\t\tif epoch % display_step == 0:\n",
    "\t\t\tprint(\"Epoch:\\t\", '%04d' % (epoch), \"\\tcost =\", \"{:.9f}\".format(avg_cost))\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\tprint(\"\\nTraining Finished!\\n\")\n",
    "\n",
    "\n",
    "\t# Testing method - Will return true if the predicted value matches the label\n",
    "\tcorrect_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "\n",
    "\n",
    "\t\n",
    "\tsucc = 0.0\t# initialize how many accurate answers there are\n",
    "\t\n",
    "\tprint(\"Predicted Output\\t\\tTargetOutput\\tSuccess\\n\")\n",
    "\n",
    "\t# loops through the test cases\n",
    "\tfor i in range(len(testing_data)):\n",
    "\t\tprint(sess.run(pred, feed_dict={x: testing_data[i], y: testing_labels[i]}), end=\"\\t\")\n",
    "\t\tprint(sess.run(y, feed_dict={x: testing_data[i], y: testing_labels[i]}), end=\"\\t\")\n",
    "\t\tout = sess.run(correct_prediction, feed_dict={x: testing_data[i], y: testing_labels[i]})\n",
    "\t\tprint(out)\n",
    "\t\tprint()\n",
    "\t\tif(out[0] == True):\t# increment counter if model corrected correctly\n",
    "\t\t\tsucc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\t0.65\n",
      "\n",
      "Time elapsed:  535.4320667031164\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy:\\t{0}\".format(succ / len(testing_data)))\n",
    "stop = timeit.default_timer()\t# stop the timer\n",
    "print(\"\\nTime elapsed: \", stop - start)\n",
    "\n",
    "\n",
    "# close the textfiles\n",
    "train_txt.close()\n",
    "test_txt.close()\n",
    "train_labels_txt.close()\n",
    "test_labels_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
